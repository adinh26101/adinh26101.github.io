---
title: "Tá»‘i Æ°u Ä‘á»“ng bá»™ vÃ  Ä‘ÃªÌ€ xuÃ¢Ìt kiáº¿n trÃºc microservice cho Split Federated Learning"
date: 2025-02-12 10:00:00 +0000
categories: [Deep Learning, Split Federated Learning, Docker, Microservice]
tags: [Deep Learning, Split Federated Learning, Docker, Microservice, Split Learning, GPU Container, File-Based Gradient, Synchronous]
author: adinh26101
icon: fas fa-ruler
lang: vi
permalink: /posts/micro-sfl/
math: true
pin: true
---

### Ná»™i dung
- [1. Giá»›i thiá»‡u](#-gioi-thieu)
- [2. Split Federated Learning: PhÃ¢n tiÌch lan truyÃªÌ€n xuÃ´i vaÌ€ ngÆ°Æ¡Ì£c](#-forward-backward)
- [3. File-based Method giuÌp xá»­ lÃ½ song song vÃ  truyá»n gradient á»•n Ä‘á»‹nh trong Split Learning](#-file-based)
- [4. KiÃªÌn truÌc microservice cho Split Federated Learning](#-microservice)

<a name="-gioi-thieu"></a>
## 1. Giá»›i thiá»‡u

Federated Learning lÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y cá»™ng tÃ¡c phÃ¢n tÃ¡n cho phÃ©p mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n mÃ  khÃ´ng cáº§n chia sáº» dá»¯ liá»‡u ngÆ°á»i dÃ¹ng. KhÃ¡i niá»‡m nÃ y Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi nhÃ³m nghiÃªn cá»©u Google do Brendan McMahan dáº«n dáº¯t trong bÃ i bÃ¡o kinh Ä‘iá»ƒn nÄƒm 2016 *â€œCommunication-Efficient Learning of Deep Networks from Decentralized Dataâ€*. Tá»« Ä‘Ã³, Federated Learning nhanh chÃ³ng trá»Ÿ thÃ nh chá»§ Ä‘á» ná»•i báº­t nhá» kháº£ nÄƒng há»c phÃ¢n tÃ¡n Ä‘i kÃ¨m báº£o máº­t dá»¯ liá»‡u.

Split Federated Learning (SFL) lÃ  má»™t biáº¿n thá»ƒ cá»§a Federated Learning, Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Chandra Thapa vÃ  cá»™ng sá»± trong bÃ i bÃ¡o *â€œSplitFed: When Federated Learning Meets Split Learningâ€* (AAAI 2022). ÄÃ¢y lÃ  nhÃ³m Ä‘áº§u tiÃªn Ä‘á»‹nh nghÄ©a SFL nhÆ° mÃ´ hÃ¬nh káº¿t há»£p giá»¯a Split Learning vÃ  Federated Learning. Äiá»ƒm máº¡nh cá»§a SFL náº±m á»Ÿ kháº£ nÄƒng chia nhá» mÃ´ hÃ¬nh, giáº£m táº£i vÃ  chi phÃ­ tÃ­nh toÃ¡n cho client/node â€” Ä‘áº·c biá»‡t phÃ¹ há»£p khi thiáº¿t bá»‹ Ä‘áº§u cuá»‘i háº¡n cháº¿ tÃ i nguyÃªn hoáº·c khÃ´ng thá»ƒ xá»­ lÃ½ mÃ´ hÃ¬nh lá»›n.

Nhiá»u nghiÃªn cá»©u gáº§n Ä‘Ã¢y hÆ°á»›ng tá»›i tá»‘i Æ°u hÃ³a SFL Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t vÃ  kháº£ nÄƒng triá»ƒn khai thá»±c táº¿, nhÆ°ng váº«n cÃ²n tá»“n táº¡i háº¡n cháº¿ nhÆ° yÃªu cáº§u duy trÃ¬ graph retention khi cáº­p nháº­t mÃ´ hÃ¬nh. Trong bÃ i nÃ y, mÃ¬nh Ä‘á» xuáº¥t má»™t cÆ¡ cháº¿ cáº­p nháº­t trá»ng sá»‘ má»›i cÃ¹ng kiáº¿n trÃºc microservice giÃºp triá»ƒn khai SFL hiá»‡u quáº£ hÆ¡n trong mÃ´i trÆ°á»ng thá»±c hoáº·c trÃªn ná»n táº£ng Ä‘Ã¡m mÃ¢y.

![Split Federated Learning Process](assets/2025-12-02-micro-sfl/sfl_base_process.gif)
*Quy trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a Spit Federated Learning*

Split Federated Learning (SFL) lÃ  mÃ´ hÃ¬nh káº¿t há»£p giá»¯a Federated Learning vÃ  Split Learning, trong Ä‘Ã³ mÃ´ hÃ¬nh Ä‘Æ°á»£c chia thÃ nh hai pháº§n: **client encoder** vá»›i trá»ng sá»‘ $W_c$ vÃ  **server backbone** vá»›i trá»ng sá»‘ $W_s$. Má»—i client xá»­ lÃ½ dá»¯ liá»‡u cá»¥c bá»™ qua encoder $f_c(x; W_c)$ Ä‘á»ƒ táº¡o activation trung gian $h$, gá»­i lÃªn server Ä‘á»ƒ tiáº¿p tá»¥c forward vÃ  nháº­n láº¡i gradient Ä‘á»ƒ cáº­p nháº­t. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, client chá»‰ cáº­p nháº­t $W_c$ báº±ng gradient cá»¥c bá»™, cÃ²n server cáº­p nháº­t $W_s$ báº±ng gradient tá»•ng há»£p tá»« toÃ n bá»™ client. Sá»± tÃ¡ch mÃ´ hÃ¬nh cÃ³ thá»ƒ mÃ´ táº£ ngáº¯n gá»n báº±ng:

$$
f(x; W) = f_s\big(f_c(x; W_c); W_s\big), 
\qquad W = \{W_c, W_s\}.
$$

![Split Federated Learning Config](assets/2025-12-02-micro-sfl/split_setting.jpg)
*Kiáº¿n trÃºc SL vá»›i nhÃ£n Ä‘Æ°á»£c giá»¯ láº¡i hoÃ n toÃ n á»Ÿ phÃ­a client*

Nhá» cÃ¡ch chia nÃ y, SFL giáº£m táº£i tÃ­nh toÃ¡n cho client, váº«n Ä‘áº£m báº£o quyá»n riÃªng tÆ° vÃ  dá»… dÃ ng má»Ÿ rá»™ng trÃªn mÃ´i trÆ°á»ng phÃ¢n tÃ¡n.

<a name="-forward-backward"></a>
## 2. Split Federated Learning: PhÃ¢n tiÌch lan truyÃªÌ€n xuÃ´i vaÌ€ ngÆ°Æ¡Ì£c

**Quy Æ°Æ¡Ìc kyÌ hiÃªÌ£u**
- **$ x_j^i \in \mathbb{R}^{d} $**: máº«u Ä‘áº§u vÃ o thá»© $j$ cá»§a client $i$, vá»›i $j = 1,\dots,n^i$ vÃ  $i = 1,\dots,M$.
- **$f_c$**: encoder phÃ­a client (Ä‘Ã³ng bÄƒng), táº¡o activation $h \in \mathbb{R}^{d_c}$.
- **$f_s(\mathbf{W}_s)$**: mÃ´-Ä‘un forward phÃ­a server vá»›i trá»ng sá»‘ $\mathbf{W}_s$, táº¡o Ä‘áº§u ra $z \in \mathbb{R}^{d_s}$.
- **$f_h(\mathbf{W}_c)$**: head phÃ­a client (trainable) vá»›i trá»ng sá»‘ $\mathbf{W}_c$, táº¡o dá»± Ä‘oÃ¡n $\hat{y} \in \mathbb{R}^{C}$.
- **$y$**: nhÃ£n tháº­t, loss Ä‘Æ°á»£c tÃ­nh báº±ng $\ell = L(\hat{y}, y)$.

---

**Lan truyÃªÌ€n xuÃ´i (Forward pass)**

Input $x_j^i$ Ä‘i qua encoder phÃ­a client, backbone server vÃ  head phÃ­a client Ä‘á»ƒ táº¡o dá»± Ä‘oÃ¡n $\hat{y}_j^i$ vÃ  loss $\ell_j^i$:

$$
x_j^i \xrightarrow{f_c} h_j^i 
\xrightarrow{f_s(\mathbf{W}_s)} z_j^i 
\xrightarrow{f_h(\mathbf{W}_c^i)} \hat{y}_j^i,
\qquad
\ell_j^i = L(\hat{y}_j^i, y_j^i)
$$

---

**Lan truyÃªÌ€n ngÆ°Æ¡Ì£c (Backward pass)**

Gradient cho head phÃ­a client:

$$
\mathbf{g}_{c,j}^i = \frac{\partial \ell_j^i}{\partial \mathbf{W}_c^i}
$$

Gradient tráº£ vá» server:

$$
\delta_j^i = \frac{\partial \ell_j^i}{\partial z_j^i}
$$

Gradient cho backbone server:

$$
\mathbf{g}_{s,j}^i 
= \frac{\partial \ell_j^i}{\partial \mathbf{W}_s}
= \delta_j^i \cdot \frac{\partial z_j^i}{\partial \mathbf{W}_s}
$$

>ğŸ’¡ **Note:** Äá»ƒ cáº­p nháº­t backbone phÃ­a Split Server, ta cáº§n gradient $$\mathbf{g}_{s,j}^i$$, vÃ  giÃ¡ trá»‹ nÃ y phá»¥ thuá»™c vÃ o hai thÃ nh pháº§n: (1) gradient cá»§a loss theo Ä‘áº§u ra server $$\delta_j^i = \frac{\partial \ell_j^i}{\partial z_j^i}$$, Ä‘Æ°á»£c client gá»­i vá» trong backward pass; (2) Ä‘áº¡o hÃ m cá»§a Ä‘áº§u ra server theo trá»ng sá»‘ $$\frac{\partial z_j^i}{\partial \mathbf{W}_s}$$, Ä‘Æ°á»£c server tá»± tÃ­nh vÃ¬ nÃ³ sá»Ÿ há»¯u mÃ´-Ä‘un $$f_s$$. Káº¿t há»£p hai pháº§n theo chain rule, server thu Ä‘Æ°á»£c gradient $$\mathbf{g}_{s,j}^i = \delta_j^i \cdot \frac{\partial z_j^i}{\partial \mathbf{W}_s}$$.


---

**Batch-wise and Parallel Updates**

Cáº­p nháº­t client head:

$$
\mathbf{W}_c^i \leftarrow 
\mathbf{W}_c^i 
- \eta \frac{\sum_{j=1}^{n^i} \mathbf{g}_{c,j}^i}{n^i}
$$

Cáº­p nháº­t server backbone:

$$
\mathbf{W}_s \leftarrow 
\mathbf{W}_s 
- \eta \frac{\sum_{i=1}^{M} \sum_{j=1}^{n^i} \mathbf{g}_{s,j}^i}{\sum_{i=1}^{M} n^i}
$$

---

**Federated Aggregation**

$$
\mathbf{W}_c^{\text{fed}}
=
\sum_{i=1}^{M}
\frac{n^i}{\sum_{i=1}^{M} n^i}
\mathbf{W}_c^i
$$

Sau Ä‘Ã³ phÃ¢n phá»‘i láº¡i cho táº¥t cáº£ client.

<a name="-file-based"></a>
## 3. File-based Method giuÌp xá»­ lÃ½ song song vÃ  truyá»n gradient á»•n Ä‘á»‹nh trong Split Learning

NhÆ° Ä‘Ã£ phÃ¢n tÃ­ch á»Ÿ pháº§n trÃªn, Ä‘á»ƒ tÃ­nh gradient cho má»—i batch, phÃ­a server chá»‰ cáº§n hai yáº¿u tá»‘: giÃ¡ trá»‹ $$z_j^i$$ sau forward vÃ  gradient cá»§a loss theo $$z_j^i$$ (tá»©c $$\delta_j^i = \frac{\partial \ell_j^i}{\partial z_j^i}$$) mÃ  client gá»­i tráº£ vá». Do Ä‘Ã³, sau khi tÃ­nh xong $$z_j^i$$, server chá»‰ cáº§n lÆ°u láº¡i giÃ¡ trá»‹ nÃ y theo dáº¡ng file rá»“i gá»­i $$z_j^i$$ vá» client. Khi nháº­n Ä‘Æ°á»£c $$\delta_j^i$$, server chá»‰ cáº§n Ä‘á»c láº¡i file tÆ°Æ¡ng á»©ng vÃ  thá»±c hiá»‡n tÃ­nh gradient cho backbone. Äá»ƒ trÃ¡nh trÃ¹ng láº·p hoáº·c xung Ä‘á»™t file trong mÃ´i trÆ°á»ng xá»­ lÃ½ song song, má»—i forward Ä‘Æ°á»£c gÃ¡n má»™t UUID4 duy nháº¥t. CÃ¡ch lÃ m nÃ y giÃºp loáº¡i bá» hoÃ n toÃ n nhu cáº§u giá»¯ láº¡i computational graph trÃªn server, tá»« Ä‘Ã³ giáº£i quyáº¿t Ä‘iá»ƒm ngháº½n lá»›n cá»§a Split Federated Learning trong giai Ä‘oáº¡n backward.

![File-based gradient for Split Federated Learning](assets/2025-12-02-micro-sfl/sfl_propose_process.gif)
*Kiáº¿n trÃºc SFL sá»­ dá»¥ng cÆ¡ cháº¿ file-based Ä‘á»ƒ lÆ°u kÃ­ch hoáº¡t vÃ  tÃ­nh gradient an toÃ n, khÃ´ng cáº§n giá»¯ graph*

<a name="-microservice"></a>
## 4. KiÃªÌn truÌc microservice cho Split Federated Learning

HiÌ€nh bÃªn dÆ°Æ¡Ìi minh hoÌ£a caÌc thaÌ€nh phÃ¢Ì€n cÃ¢Ì€n thiÃªÌt cho hÃªÌ£ thÃ´Ìng. NgoaÌ€i client, split server vaÌ€ federated server, mÃ´Ì£t backend Ä‘Æ°Æ¡Ì£c thÃªm vaÌ€o Ä‘ÃªÌ‰ Ä‘iÃªÌ€u phÃ´Ìi quaÌ triÌ€nh hoÌ£c, cuÌ€ng database lÆ°u metadata vaÌ€ dashboard theo doÌƒi realtime.

![System Components](assets/2025-12-02-micro-sfl/system_component.png)
*CÃ¡c thÃ nh pháº§n cá»§a há»‡ thá»‘ng*

Backend seÌƒ lÄƒÌng nghe caÌc service khaÌc Ä‘ÃªÌ‰ Ä‘iÃªÌ€u phÃ´Ìi quaÌ triÌ€nh hoÌ£c. MÃ´Ì£t voÌ€ng hoÌ£c cuÌ‰a mÃ´Ì£t client diÃªÌƒn ra nhÆ° sau:

![One Global Round Process](assets/2025-12-02-micro-sfl/one_global_round_process.jpg)
*MÃ´Ì£t voÌ€ng hoÌ£c toaÌ€n cuÌ£c*

CÆ¡ chÃªÌ cÃ¢Ì£p nhÃ¢Ì£t file-based gradient cho pheÌp split server chiÌ‰ cÃ¢Ì€n lÆ°u troÌ£ng sÃ´Ì vaÌ€ thÆ°Ì£c hiÃªÌ£n forward/inference, viÃªÌ£c cÃ¢Ì£p nhÃ¢Ì£t chiÌ‰ cÃ¢Ì€n trung biÌ€nh gradient tÆ°Ì€ caÌc file. Khi sÃ´Ì client tÄƒng, gradient coÌ thÃªÌ‰ Ä‘Æ°Æ¡Ì£c Ä‘Ã¢Ì‰y vaÌ€o storage chung, giuÌp split server trÆ¡Ì‰ thaÌ€nh stateless vaÌ€ scale horizontally nhÆ° sau:

![Scale Horizontal](assets/2025-12-02-micro-sfl/scale_horizontal.jpg)
*SFL scale horizontally*

NgoaÌ€i ra, coÌ thÃªÌ‰ khai thaÌc container hÃ´Ìƒ trÆ¡Ì£ GPU Ä‘ÃªÌ‰ tÄƒng tÃ´Ìc cho caÌc service cÃ¢Ì€n tiÌnh toaÌn. Trong thiÌ nghiÃªÌ£m, miÌ€nh chaÌ£y trÃªn Windows, caÌ€i NVIDIA driver, CUDA Toolkit, NVIDIA Container Toolkit vaÌ€ sÆ°Ì‰ duÌ£ng image `pytorch/pytorch:2.2.2-cuda12.1-cudnn8-devel`. KiÃªÌn truÌc Ä‘Æ°Æ¡Ì£c mÃ´ taÌ‰ nhÆ° hiÌ€nh bÃªn dÆ°Æ¡Ìi:

![GPU Container Architecture](assets/2025-12-02-micro-sfl/gpu_container_architecture.jpg)
*KiÃªÌn truÌc GPU container*

BaÌ£n coÌ thÃªÌ‰ Ä‘oÌ£c thÃªm vaÌ€ tham khaÌ‰o vÃªÌ€ thiÌ nghiÃªÌ£m trong baÌ€i baÌo naÌ€y: [táº¡i Ä‘Ã¢y]().

Code sÆ°Ì‰ duÌ£ng cho thiÌ nghiÃªÌ£m: [táº¡i Ä‘Ã¢y](https://github.com/frogdance/MicroSFL).